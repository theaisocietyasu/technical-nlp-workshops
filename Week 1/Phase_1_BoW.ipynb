{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4zB3mgGhVhTo"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wT9VreKB--YJ"
   },
   "outputs": [],
   "source": [
    "def unzip_file(zip_path, extract_to='.'):\n",
    "    #unziping to a directory\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6MOsY15t_ClW"
   },
   "outputs": [],
   "source": [
    "def read_text_files(directory):\n",
    "    # List to store the content of each file\n",
    "    text_files_content = []\n",
    "\n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                # Read the content and store it\n",
    "                text_files_content.append(file.read())\n",
    "\n",
    "    return text_files_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4DbcNbcM_XJf"
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    # Splitting the text by spaces to get individual words\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gO5sCzPo_Y_S"
   },
   "outputs": [],
   "source": [
    "def to_lowercase(tokens):\n",
    "    # Convert each token to lowercase\n",
    "    return [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nUQKu_aW_hDN"
   },
   "outputs": [],
   "source": [
    "stop_words = ['the', 'and', 'is', 'in', 'at', 'of', 'a', 'an']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "D_t-JFj1_a_2"
   },
   "outputs": [],
   "source": [
    "# Remove tokens that are in the stop_words list\n",
    "def remove_stop_words(tokens, stop_words):\n",
    "    return [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D-jH3qSbALfC"
   },
   "outputs": [],
   "source": [
    "# Remove common suffixes from tokens\n",
    "def simple_stemmer(tokens):\n",
    "    suffixes = ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']\n",
    "\n",
    "    stemmed_tokens = []\n",
    "    for token in tokens:\n",
    "        for suffix in suffixes:\n",
    "            if token.endswith(suffix):\n",
    "                token = token[:-len(suffix)]\n",
    "        stemmed_tokens.append(token)\n",
    "\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Gw1sNZ87A1VM"
   },
   "outputs": [],
   "source": [
    "# Replace tokenbs with their base form using a dictionary\n",
    "def simple_lemmatizer(tokens):\n",
    "    lemma_dict = {\n",
    "        'better': 'good',\n",
    "        'ran': 'run',\n",
    "        'running': 'run',\n",
    "        'cats': 'cat'\n",
    "    }\n",
    "\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        # Replace with the base form if it exists in lemma_dict\n",
    "        if token in lemma_dict:\n",
    "            lemmatized_tokens.append(lemma_dict[token])\n",
    "        else:\n",
    "            lemmatized_tokens.append(token)\n",
    "\n",
    "    return lemmatized_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cxi0hykOA3hd"
   },
   "outputs": [],
   "source": [
    "# Create a vocabulary of unique words\n",
    "def build_vocabulary(tokens):\n",
    "    vocabulary = {}\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xxYoapnVA7Zl"
   },
   "outputs": [],
   "source": [
    "def vectorize_text(tokens, vocabulary):\n",
    "    # Convert text into a vector based on word counts ( Bag of Words )\n",
    "    vector = [0] * len(vocabulary)\n",
    "    for token in tokens:\n",
    "        if token in vocabulary:\n",
    "            vector[vocabulary[token]] += 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nUjmvBCLA_Zq"
   },
   "outputs": [],
   "source": [
    "zip_path = 'archive.zip'\n",
    "extract_to = 'archive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3Gh7_EmjBMWh"
   },
   "outputs": [],
   "source": [
    "unzip_file(zip_path, extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5gwoyyBsBXa3"
   },
   "outputs": [],
   "source": [
    "texts = read_text_files(extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "daEBLW5DBeNK",
    "outputId": "9d21aa6a-de4f-4a53-ae66-59313007a396",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Text file 1:\n",
      "Tokens: ['newsgroup:', 'sci.crypt', 'document_id:', '14147', 'from:', 'marc', 'vanheyningen', '<mvanheyn@cs.indiana.edu>', 'subject:', 'ripem']\n",
      "Vector: [1984, 2070, 1982, 2, 2126, 88, 34, 18, 2000, 216]\n",
      "Vocabulary: 32208\n",
      "Processing Text file 2:\n",
      "Tokens: ['newsgroup:', 'comp.sys.mac.hardware', 'document_id:', '50418', 'from:', 'xor@clotho.acm.rpi.edu', '(joe', 'schwartz)', 'subject:', 're:']\n",
      "Vector: [1922, 1934, 1922, 2, 1940, 4, 4, 4, 1940, 1076]\n",
      "Vocabulary: 22585\n",
      "Processing Text file 3:\n",
      "Tokens: ['newsgroup:', 'misc.forsale', 'document_id:', '70337', 'from:', 'kedz@bigwpi.wpi.edu', '(john', 'kedziora)', 'subject:', 'motorcycle']\n",
      "Vector: [1944, 1952, 1944, 2, 1956, 2, 46, 2, 1978, 4]\n",
      "Vocabulary: 24556\n",
      "Processing Text file 4:\n",
      "Tokens: ['newsgroup:', 'soc.religion.christian', 'document_id:', '20361', 'from:', 'jenk@microsoft.com', '(jen', 'kilmer)', 'subject:', 're:']\n",
      "Vector: [2004, 2020, 1994, 2, 2006, 20, 12, 12, 1998, 1522]\n",
      "Vocabulary: 34652\n",
      "Processing Text file 5:\n",
      "Tokens: ['newsgroup:', 'rec.sport.baseball', 'document_id:', '100521', 'from:', 'admiral@jhunix.hcf.jhu.edu', '(steve', 'c', 'liu)', 'subject:']\n",
      "Vector: [1988, 2006, 1988, 2, 1992, 18, 54, 110, 18, 1990]\n",
      "Vocabulary: 26369\n",
      "Processing Text file 6:\n",
      "Tokens: ['newsgroup:', 'rec.sport.hockey', 'document_id:', '52550', 'from:', 'dchhabra@stpl.ists.ca', '(deepak', 'chhabra)', 'subject:', 'superstar']\n",
      "Vector: [1998, 2102, 1998, 2, 2004, 98, 68, 68, 2006, 22]\n",
      "Vocabulary: 32293\n",
      "Processing Text file 7:\n",
      "Tokens: ['newsgroup:', 'comp.sys.ibm.pc.hardware', 'document_id:', '58343', 'from:', 'bobmon@cs.indiana.edu', '(bob', 'montante)', 'subject:', 'wanted:']\n",
      "Vector: [1964, 1986, 1964, 2, 1994, 4, 20, 2, 2002, 18]\n",
      "Vocabulary: 24668\n",
      "Processing Text file 8:\n",
      "Tokens: ['newsgroup:', 'talk.politics.gun', 'document_id:', '53293', 'from:', 'manes@magpie.linknet.com', '(steve', 'manes)', 'subject:', 're:']\n",
      "Vector: [1820, 1850, 1820, 2, 1842, 64, 106, 50, 1834, 1680]\n",
      "Vocabulary: 30445\n",
      "Processing Text file 9:\n",
      "Tokens: ['newsgroup:', 'rec.auto', 'document_id:', '101551', 'from:', 'cs012055@cs.brown.edu', '(hok-chung', 'tsang)', 'subject:', 're:']\n",
      "Vector: [1980, 2024, 1980, 2, 2000, 6, 6, 6, 1998, 1592]\n",
      "Vocabulary: 26168\n",
      "Processing Text file 10:\n",
      "Tokens: ['from:', 'mathew', '<mathew@mantis.co.uk>', 'subject:', 'alt.atheism', 'faq:', 'atheist', 'resourc', 'archive-name:', 'atheism/resourc']\n",
      "Vector: [2424, 270, 153, 2454, 1784, 60, 1257, 54, 21, 6]\n",
      "Vocabulary: 27777\n",
      "Processing Text file 11:\n",
      "Tokens: ['newsgroup:', 'comp.os.ms-windows.misc', 'document_id:', '10000', 'from:', 'yeoy@a.cs.okstate.edu', '(yeo', 'yek', 'chong)', 'subject:']\n",
      "Vector: [1970, 1980, 1970, 2, 1992, 2, 2, 4, 2, 1982]\n",
      "Vocabulary: 46293\n",
      "Processing Text file 12:\n",
      "Tokens: ['newsgroup:', 'sci.electronic', 'document_id:', '52434', 'from:', 'et@teal.csn.org', '(eric', 'h.', 'taylor)', 'subject:']\n",
      "Vector: [1966, 1980, 1962, 2, 1988, 6, 28, 40, 6, 2076]\n",
      "Vocabulary: 25295\n",
      "Processing Text file 13:\n",
      "Tokens: ['newsgroup:', 'comp.windows.x', 'document_id:', '64830', 'from:', 'chongo@toad.com', '(landon', 'c.', 'noll)', 'subject:']\n",
      "Vector: [1960, 2026, 1960, 2, 2006, 24, 8, 84, 8, 2402]\n",
      "Vocabulary: 37793\n",
      "Processing Text file 14:\n",
      "Tokens: ['newsgroup:', 'talk.religion.misc', 'document_id:', '82757', 'from:', 'dsoconne@quads.uchicago.edu', '(daniel', '', 'oconnell)', 'subject:']\n",
      "Vector: [1258, 1268, 1256, 2, 1268, 2, 8, 54, 2, 1264]\n",
      "Vocabulary: 26448\n",
      "Processing Text file 15:\n",
      "Tokens: ['newsgroup:', 'talk.politics.mideast', 'document_id:', '75364', 'from:', 'hasan@mcrcim.mcgill.edu', 'subject:', 're:', 'islam', 'borders.']\n",
      "Vector: [1882, 1888, 1880, 2, 1960, 48, 1976, 1432, 140, 14]\n",
      "Vocabulary: 37851\n",
      "Processing Text file 16:\n",
      "Tokens: ['newsgroup:', 'sci.m', 'document_id:', '57110', 'from:', 'bed@intacc.uucp', '(deb', 'waddington)', 'subject:', 'info']\n",
      "Vector: [1980, 2028, 1980, 2, 2050, 2, 2, 2, 1996, 188]\n",
      "Vocabulary: 34421\n",
      "Processing Text file 17:\n",
      "Tokens: ['newsgroup:', 'rec.motorcycl', 'document_id:', '101725', 'subject:', 're:', 'lexan', 'polish?', 'from:', 'jeff@mri.com']\n",
      "Vector: [1988, 2036, 1988, 2, 1992, 1756, 2, 2, 2000, 6]\n",
      "Vocabulary: 24632\n",
      "Processing Text file 18:\n",
      "Tokens: ['from:', 'lipman@oasys.dt.navy.mil', '(robert', 'lipman)', 'subject:', 'call', 'for', 'presentations:', 'navy', 'sciviz/vr']\n",
      "Vector: [2617, 18, 95, 6, 2502, 527, 7393, 15, 24, 6]\n",
      "Vocabulary: 31213\n",
      "Processing Text file 19:\n",
      "Tokens: ['newsgroup:', 'sci.space', 'document_id:', '59497', 'from:', 'et@teal.csn.org', '(eric', 'h.', 'taylor)', 'subject:']\n",
      "Vector: [1974, 2060, 1974, 2, 2040, 6, 18, 38, 6, 1992]\n",
      "Vocabulary: 34466\n",
      "Processing Text file 20:\n",
      "Tokens: ['newsgroup:', 'talk.politics.misc', 'document_id:', '124146', 'from:', 'mpye@vmsb.is.csupomona.edu', 'subject:', 're:', 'media', 'horrifi']\n",
      "Vector: [1550, 1552, 1550, 2, 1580, 2, 1568, 1512, 136, 4]\n",
      "Vocabulary: 31537\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(texts):\n",
    "    print(f\"Processing Text file {i+1}:\")\n",
    "\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = to_lowercase(tokens)\n",
    "    tokens = remove_stop_words(tokens, stop_words)\n",
    "    tokens = simple_stemmer(tokens)\n",
    "    tokens = simple_lemmatizer(tokens)\n",
    "\n",
    "    vocabulary = build_vocabulary(tokens)\n",
    "    vector = vectorize_text(tokens, vocabulary)\n",
    "\n",
    "    print(\"Tokens:\", tokens[:10])\n",
    "    print(\"Vector:\", vector[:10])\n",
    "    print(\"Vocabulary:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
